{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fd3784abee0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id=\"blocks.8.hook_resid_pre\",  # won't always be a hook point\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_abl_hook(activation, hook):\n",
    "#     return torch.zeros_like(activation)\n",
    "\n",
    "def direct(activation, hook):\n",
    "    return activation\n",
    "\n",
    "def clamping(activation, hook):\n",
    "    encoded = sae.encode(activation)\n",
    "\n",
    "    # The activations are now stored in intermediate_outputs\n",
    "    # Modify activations\n",
    "    # print(encoded.shape)\n",
    "    # print(encoded[:, :, torch.argmax(encoded)])\n",
    "    # encoded[:, :, :] = 0.0\n",
    "    encoded[0, :, 123] = 50.0\n",
    "\n",
    "    sae_out = sae.decode(encoded)\n",
    "\n",
    "    return sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8d81a29d94446991eb4ff830599c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>It is safest when the flow in a pipe is relatively high and continuous, such as when welding up steel, providing flexible welds and the than midway impact, where resistance is very high. FUSE operates primarily because of its high hydraulic strength, which means that over suction induced pressure is not communicated to resistive material. This makes FUSE safer to use when welding blocks on other types of pipes, especially those that have originated on pipes that draw less than one-half inch downward. Smaller blocks, or presses, or pipes with lower\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ab2326bf2e4a359a00017fd718c70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>It is safest when the flow in a pipe is crowded and affordable to Stone's Weekly Harvest\n",
      "\n",
      "Camp Phays Central Middle School hosted The Student Center North Center\n",
      "\n",
      "Anti-Hot Spring Music Festival begins\n",
      "\n",
      "Phary Ball Sony Music Festival, TBA\n",
      "\n",
      "Crystal Creek In 844 Distillier Good Central Per Fill Cango De Praeda Print Garden The Big East ALFC The Cellos Amphitheater Amphitheatre Diana Arena Caes Clayer from The Bottom Down Auditorium\n",
      "\n",
      "American Renaissance Festival Chesapeake Children's Coliseum\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_prompt = \"It is safest when the flow in a pipe is\"\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[(sae.cfg.hook_name, direct)],\n",
    "):\n",
    "    \n",
    "    input_tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "\n",
    "    generated_tokens = model.generate(input_tokens, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "    generated_tokens = model.tokenizer.decode(generated_tokens[0])\n",
    "    print(generated_tokens)\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[(sae.cfg.hook_name, clamping)],\n",
    "):\n",
    "    input_tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "\n",
    "    generated_tokens = model.generate(input_tokens, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "    generated_tokens = model.tokenizer.decode(generated_tokens[0])\n",
    "    print(generated_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
