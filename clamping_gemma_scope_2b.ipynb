{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8260ab7fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b11c8d414343069dc2f94b95fdb41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
      "Model loaded successfully\n",
      "SAE loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res-canonical\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id=\"layer_12/width_16k/canonical\",  # won't always be a hook point\n",
    "    device=device,\n",
    ")\n",
    "print(\"SAE loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_abl_hook(activation, hook):\n",
    "#     return torch.zeros_like(activation)\n",
    "\n",
    "def direct(activation, hook):\n",
    "    return activation\n",
    "\n",
    "def clamping(activation, hook):\n",
    "    encoded = sae.encode(activation)\n",
    "\n",
    "    # The activations are now stored in intermediate_outputs\n",
    "    # Modify activations\n",
    "    # print(encoded.shape)\n",
    "    # print(encoded[:, :, torch.argmax(encoded)])\n",
    "    # encoded[:, :, :] = 0.0\n",
    "    encoded[0, :, 6] = 150.0\n",
    "\n",
    "    sae_out = sae.decode(encoded)\n",
    "\n",
    "    return sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298640768e3c409694bef0cfcbde4991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Between dog and an fish, a dolphin is closer to a shark. That is a more logical explanation. And not one that makes any sense whatsoever.\n",
      "Because... they're sharks.\n",
      "Because they are marine life and the shark lives in water.\n",
      "Probably because it lives in the ocean, or something like that.\n",
      "Because they are similar... it may not make sense, but just think about it. Kangaroos look nothing and similar to a lamp but a lamp and a kangaroo are animals. Watch how there two feet and legs move in the same\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db700998a6f496c94d6622a23387fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Between dog and an fish, a dolphin is closer to a Fish.\n",
      "Between Dog and, I\n",
      "Between Dog and I\n",
      "I Watch III and I Watch I\n",
      "I Watch I\n",
      "I Watch I\n",
      "I Watch I Watch,andelson I Watch I\n",
      "\n",
      "The glory of which the gun.\n",
      "I watch the Moon.\n",
      "Watch I\n",
      "Thet and I watch and I\n",
      "I\n",
      "I\n",
      "I\n",
      "Iey\n",
      "i\n",
      "I\n",
      "i\n",
      "I\n",
      "i\n",
      "y\n",
      "\n",
      "I\n",
      "I\n",
      "NAMES\n",
      "\n",
      "I\n",
      "\n",
      "I, I\n",
      "\n",
      "<h6>\n"
     ]
    }
   ],
   "source": [
    "example_prompt = \"Between dog and an fish, a dolphin is closer to a\"\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[(sae.cfg.hook_name, direct)],\n",
    "):\n",
    "    \n",
    "    input_tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "\n",
    "    generated_tokens = model.generate(input_tokens, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "    generated_tokens = model.tokenizer.decode(generated_tokens[0])\n",
    "    print(generated_tokens)\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[(sae.cfg.hook_name, clamping)],\n",
    "):\n",
    "    input_tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "\n",
    "    generated_tokens = model.generate(input_tokens, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "    generated_tokens = model.tokenizer.decode(generated_tokens[0])\n",
    "    print(generated_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
